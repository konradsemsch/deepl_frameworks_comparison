{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html\n",
    "\n",
    "import mxnet as mx\n",
    "\n",
    "# Fixing the random seed\n",
    "mx.random.seed(42)\n",
    "\n",
    "mnist = mx.test_utils.get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_data = mx.io.NDArrayIter(\n",
    "    mnist['train_data'], \n",
    "    mnist['train_label'], \n",
    "    batch_size, \n",
    "    shuffle = True # better to randomly shuffle training data to ensure that same labels are not feeded consequently\n",
    ")\n",
    "\n",
    "val_data = mx.io.NDArrayIter(\n",
    "    mnist['test_data'], \n",
    "    mnist['test_label'], \n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture\n",
    "net = nn.Sequential()\n",
    "\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(128, activation = 'relu'))\n",
    "    net.add(nn.Dense(64, activation = 'relu'))\n",
    "    net.add(nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 0)\n"
     ]
    }
   ],
   "source": [
    "gpus = mx.test_utils.list_gpus()\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = [mx.gpu()] if gpus else [mx.cpu(0), mx.cpu(1)]\n",
    "net.initialize(mx.init.Xavier(magnitude = 2.24), ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.02})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc at epoch 0: accuracy=0.781367\n",
      "training acc at epoch 1: accuracy=0.899733\n",
      "training acc at epoch 2: accuracy=0.914417\n",
      "training acc at epoch 3: accuracy=0.923533\n",
      "training acc at epoch 4: accuracy=0.931533\n",
      "training acc at epoch 5: accuracy=0.937000\n",
      "training acc at epoch 6: accuracy=0.942367\n",
      "training acc at epoch 7: accuracy=0.946567\n",
      "training acc at epoch 8: accuracy=0.950117\n",
      "training acc at epoch 9: accuracy=0.953367\n",
      "CPU times: user 59.7 s, sys: 6.59 s, total: 1min 6s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 10\n",
    "\n",
    "# Use Accuracy as the evaluation metric.\n",
    "metric = mx.metric.Accuracy()\n",
    "softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # Reset the train data iterator.\n",
    "    train_data.reset()\n",
    "    \n",
    "    # Loop over the train data iterator.\n",
    "    for batch in train_data:\n",
    "        \n",
    "        # Splits train data into multiple slices along batch_axis\n",
    "        # and copy each slice into a context.\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list = ctx, batch_axis = 0)\n",
    "        \n",
    "        # Splits train labels into multiple slices along batch_axis\n",
    "        # and copy each slice into a context.\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list = ctx, batch_axis = 0)\n",
    "        outputs = []\n",
    "        \n",
    "        # Inside training scope\n",
    "        with ag.record():\n",
    "            for x, y in zip(data, label):\n",
    "                z = net(x)\n",
    "                \n",
    "                # Computes softmax cross entropy loss.\n",
    "                loss = softmax_cross_entropy_loss(z, y)\n",
    "                \n",
    "                # Backpropagate the error for one iteration.\n",
    "                loss.backward()\n",
    "                outputs.append(z)\n",
    "\n",
    "        # Updates internal evaluation\n",
    "        metric.update(label, outputs)\n",
    "        \n",
    "        # Make one step of parameter update. Trainer needs to know the\n",
    "        # batch size of data to normalize the gradient by 1/batch_size.\n",
    "        trainer.step(batch.data[0].shape[0])\n",
    "\n",
    "    # Gets the evaluation result.\n",
    "    name, acc = metric.get()\n",
    "    \n",
    "    # Reset evaluation result to initial state.\n",
    "    metric.reset()\n",
    "    print('training acc at epoch %d: %s=%f'%(i, name, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc: accuracy=0.953200\n"
     ]
    }
   ],
   "source": [
    "# Use Accuracy as the evaluation metric.\n",
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "# Reset the validation data iterator.\n",
    "val_data.reset()\n",
    "\n",
    "# Loop over the validation data iterator.\n",
    "for batch in val_data:\n",
    "    \n",
    "    # Splits validation data into multiple slices along batch_axis\n",
    "    # and copy each slice into a context.\n",
    "    data = gluon.utils.split_and_load(batch.data[0], ctx_list = # Use Accuracy as the evaluation metric.\n",
    "metric = mx.metric.Accuracy()\n",
    "# Reset the validation data iterator.\n",
    "val_data.reset()\n",
    "# Loop over the validation data iterator.\n",
    "for batch in val_data:\n",
    "    # Splits validation data into multiple slices along batch_axis\n",
    "    # and copy each slice into a context.\n",
    "    data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "    # Splits validation label into multiple slices along batch_axis\n",
    "    # and copy each slice into a context.\n",
    "    label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "    outputs = []\n",
    "    for x in data:\n",
    "        outputs.append(net(x))\n",
    "    # Updates internal evaluation\n",
    "    metric.update(label, outputs)\n",
    "print('validation acc: %s=%f'%metric.get())\n",
    "assert metric.get()[1] > 0.94ctx, batch_axis = 0)\n",
    "    \n",
    "    # Splits validation label into multiple slices along batch_axis\n",
    "    # and copy each slice into a context.\n",
    "    label = gluon.utils.split_and_load(batch.label[0], ctx_list = ctx, batch_axis = 0)\n",
    "    outputs = []\n",
    "    \n",
    "    for x in data:\n",
    "        outputs.append(net(x))\n",
    "        \n",
    "    # Updates internal evaluation\n",
    "    metric.update(label, outputs)\n",
    "    \n",
    "print('validation acc: %s=%f'%metric.get())\n",
    "assert metric.get()[1] > 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.ndarray as F\n",
    "\n",
    "class Net(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Net, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # Layers created in name_scope will inherit name space\n",
    "            # from parent layer.\n",
    "            self.conv1 = nn.Conv2D(20, kernel_size = (5,5))\n",
    "            self.pool1 = nn.MaxPool2D(pool_size = (2,2), strides = (2,2))\n",
    "            self.conv2 = nn.Conv2D(50, kernel_size = (5,5))\n",
    "            self.pool2 = nn.MaxPool2D(pool_size = (2,2), strides = (2,2))\n",
    "            self.fc1 = nn.Dense(500)\n",
    "            self.fc2 = nn.Dense(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.tanh(self.conv1(x)))\n",
    "        x = self.pool2(F.tanh(self.conv2(x)))\n",
    "        # 0 means copy over size from corresponding dimension.\n",
    "        # -1 means infer size from the rest of dimensions.\n",
    "        x = x.reshape((0, -1))\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the context on GPU is available otherwise CPU\n",
    "ctx = [mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(mx.init.Xavier(magnitude = 2.24), ctx = ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc at epoch 0: accuracy=0.861983\n",
      "training acc at epoch 1: accuracy=0.937683\n",
      "training acc at epoch 2: accuracy=0.953133\n",
      "training acc at epoch 3: accuracy=0.962083\n",
      "training acc at epoch 4: accuracy=0.967867\n",
      "training acc at epoch 5: accuracy=0.971417\n",
      "training acc at epoch 6: accuracy=0.974233\n",
      "training acc at epoch 7: accuracy=0.976317\n",
      "training acc at epoch 8: accuracy=0.978400\n",
      "training acc at epoch 9: accuracy=0.979900\n"
     ]
    }
   ],
   "source": [
    "metric = mx.metric.Accuracy()\n",
    "softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # Reset the train data iterator.\n",
    "    train_data.reset()\n",
    "    \n",
    "    # Loop over the train data iterator.\n",
    "    for batch in train_data:\n",
    "        \n",
    "        # Splits train data into multiple slices along batch_axis\n",
    "        # and copy each slice into a context.\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        \n",
    "        # Splits train labels into multiple slices along batch_axis\n",
    "        # and copy each slice into a context.\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        \n",
    "        # Inside training scope\n",
    "        with ag.record():\n",
    "            for x, y in zip(data, label):\n",
    "                z = net(x)\n",
    "        \n",
    "                # Computes softmax cross entropy loss.\n",
    "                loss = softmax_cross_entropy_loss(z, y)\n",
    "                \n",
    "                # Backpropogate the error for one iteration.\n",
    "                loss.backward()\n",
    "                outputs.append(z)\n",
    "\n",
    "        # Updates internal evaluation\n",
    "        metric.update(label, outputs)\n",
    "        \n",
    "        # Make one step of parameter update. Trainer needs to know the\n",
    "        # batch size of data to normalize the gradient by 1/batch_size.\n",
    "        trainer.step(batch.data[0].shape[0])\n",
    "    \n",
    "    # Gets the evaluation result.\n",
    "    name, acc = metric.get()\n",
    "    \n",
    "    # Reset evaluation result to initial state.\n",
    "    metric.reset()\n",
    "    print('training acc at epoch %d: %s=%f'%(i, name, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc: accuracy=0.982100\n"
     ]
    }
   ],
   "source": [
    "# Use Accuracy as the evaluation metric.\n",
    "metric = mx.metric.Accuracy()\n",
    "\n",
    "# Reset the validation data iterator.\n",
    "val_data.reset()\n",
    "\n",
    "# Loop over the validation data iterator.\n",
    "for batch in val_data:\n",
    "\n",
    "    # Splits validation data into multiple slices along batch_axis\n",
    "    # and copy each slice into a context.\n",
    "    data = gluon.utils.split_and_load(batch.data[0], ctx_list = ctx, batch_axis = 0)\n",
    "    \n",
    "    # Splits validation label into multiple slices along batch_axis\n",
    "    # and copy each slice into a context.\n",
    "    label = gluon.utils.split_and_load(batch.label[0], ctx_list = ctx, batch_axis = 0)\n",
    "    outputs = []\n",
    "    \n",
    "    for x in data:\n",
    "        outputs.append(net(x))\n",
    "    \n",
    "    # Updates internal evaluation\n",
    "    metric.update(label, outputs)\n",
    "print('validation acc: %s=%f'%metric.get())\n",
    "assert metric.get()[1] > 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl_frameworks_comparison",
   "language": "python",
   "name": "deepl_frameworks_comparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
